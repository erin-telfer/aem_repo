{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference model constraint"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Test- how to create reference model for each sounding\n",
    "Erin Telfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### check constraint values in sbs code. \n",
    "\n",
    "#### how to spread values out laterally\n",
    "    #### how to relax constraints laterally\n",
    "#### how to change the number of layers\n",
    "#### define column numbers for values taken from dat. e.g. long and lat position in file\n",
    "#### enter depths manually or read from .con file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User requirement: enter file input information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## User requirement: enter the folder location where the .dat file is saved\n",
    "folder= str(r\"C:/Users/u67397/AnacondaProjects/aem/input_data/\")\n",
    "\n",
    "## User requirement: enter the name of the second .dat file (without the \".dat\" text)\n",
    "dat_name= str(\"AA140005_Line1080\")\n",
    "\n",
    "## User requirement: enter the name of the second .dat file (without the \".dat\" text)\n",
    "con_name= str(\"galeisbstdem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User requirement: enter reference model data variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## User requirement: enter the reference model conductivity specified for result file\n",
    "reference_EC1= 0.001\n",
    "\n",
    "## User requirement: enter the number of layers result file\n",
    "layers= 30\n",
    "\n",
    "##User requirement: set constraint weighting. 1% =no constraint (e.g. reference model) and 100%= hard constraint at location\n",
    "no_constraint=1\n",
    "hard_constraint=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User requirement: enter file output information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## User requirement: enter the folder location where to save files into\n",
    "\n",
    "output_folder= str(r\"C:/Users/u67397/AnacondaProjects/aem/output_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create array that contains contrained data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load .dat file and then process\n",
    "dat= pd.read_fwf(folder+dat_name+\".dat\", header=None) #load dat file\n",
    "dat=dat.set_index([2,3], drop=False) #set coordinates as index\n",
    "dat['all_sumcoord']=dat[2]+dat[3] #create a new column that combines coordinates\n",
    "datlen= len(dat.index) #create variable of dat length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract thickness data from .dat file and then process\n",
    "con = pd.read_csv (folder+con_name+\".con\", header=None) #import control file\n",
    "con.replace(regex=True,to_replace=r'\\t', value=r'', inplace=True)#remove tabs\n",
    "tlayers=con.loc[con[0].str.contains('Thickness      = ')] #create a dataframe that contains layer thickness saved in confile (.con)\n",
    "thicknessoflayers= tlayers.replace(regex=True,to_replace=r'Thickness      = ', value=r'')\n",
    "thicknessoflayers=thicknessoflayers[0].str.split((' '),expand=True).replace('',np.nan)\n",
    "thicknessoflayers.dropna(axis=1,inplace=True)\n",
    "thicknessoflayers=thicknessoflayers.reset_index(drop=True).T.reset_index(drop=True).T\n",
    "thicknessoflayers=thicknessoflayers.transpose().astype(float)\n",
    "bottomlayer=thicknessoflayers[0].iloc[-1]\n",
    "bottomlayer=np.array([bottomlayer])\n",
    "bottomlayer=pd.DataFrame(bottomlayer)\n",
    "thicknessoflayers=thicknessoflayers.append(bottomlayer).reset_index(drop=True)\n",
    "thicknessoflayers=np.cumsum(thicknessoflayers)\n",
    "thicknessoflayers=thicknessoflayers.transpose()\n",
    "thicknessoflayers=(pd.np.tile(thicknessoflayers, (datlen, 1)))\n",
    "thicknessoflayers=pd.DataFrame(thicknessoflayers)\n",
    "\n",
    "#thickness\n",
    "thickness_1= thicknessoflayers[0:1]\n",
    "thickness_1=np.array(thickness_1).squeeze()\n",
    "\n",
    "numberoflocations=list(range(layers))\n",
    "numberoflocations=np.asarray(numberoflocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create dataframe of reference EC values\n",
    "EC=pd.Series(reference_EC1)\n",
    "EC=(pd.np.tile(EC, (datlen, layers)))\n",
    "EC=pd.DataFrame(EC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#load constraint file and process \n",
    "constraint_name=str(\"input_test\") \n",
    "constraint_data= pd.read_csv(folder+constraint_name+\".csv\") #import control file\n",
    "constraint_data['const_sumcoord']=constraint_data['x']+constraint_data['y']\n",
    "constraint_data=constraint_data.set_index(['ID'], drop=False)\n",
    "\n",
    "#create data array of reference constraint values\n",
    "constraint_weighting=np.full((datlen,layers),no_constraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#coordinates\n",
    "longitude_coords=pd.DataFrame(dat.iloc[:,2:3])\n",
    "longitude_coords=longitude_coords.transpose()\n",
    "longitude_coords=np.array(longitude_coords).squeeze()\n",
    "\n",
    "latitude_coords=pd.DataFrame(dat.iloc[:,3:4])\n",
    "latitude_coords=latitude_coords.transpose()\n",
    "latitude_coords=np.array(latitude_coords).squeeze()\n",
    "latitude_array=pd.np.tile(latitude_coords, ((len(numberoflocations), 1))).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched=pd.DataFrame([0,0,0]).transpose().set_index([0])\n",
    "\n",
    "for x in range(len(constraint_data)):\n",
    "    g=(constraint_data.ID[x])\n",
    "    h=(constraint_data.const_sumcoord[x])\n",
    "    dat[str(g)]= np.abs(dat.all_sumcoord-h) \n",
    "    matched1=np.argmin(dat[str(g)])\n",
    "    e_matched=matched1[0]\n",
    "    n_matched=matched1[1]\n",
    "    matched2=pd.DataFrame([g,float(e_matched),float(n_matched)]).transpose().set_index([0]).astype(float)\n",
    "    matched=pd.concat([matched,matched2], axis=0)\n",
    "\n",
    "matched=matched.rename(columns={1:'long_matched',2:'lat_matched'}).drop([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint_data2= pd.concat([constraint_data, matched],axis=1)\n",
    "constraint_data2=constraint_data2.set_index(['long_matched','lat_matched'],drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:            (depth: 30, layers: 30)\n",
       "Coordinates:\n",
       "    longitude          float64 2.21e+05\n",
       "  * depth              (depth) float64 3.0 6.3 9.92 13.92 18.32 23.16 28.48 ...\n",
       "  * layers             (layers) int32 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ...\n",
       "Data variables:\n",
       "    ec_dataset         (depth) float64 0.05 0.05 0.05 0.001 0.001 0.001 ...\n",
       "    thickness_dataset  (layers) float64 3.0 6.3 9.92 13.92 18.32 23.16 28.48 ...\n",
       "    latitude_dataset   (depth) float64 6.867e+06 6.867e+06 6.867e+06 ...\n",
       "    weighting_dataset  (depth) int32 100 100 100 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for y in range(len(constraint_data2)):\n",
    "    EC_value1=constraint_data2['v'][y]\n",
    "    zt=constraint_data2['zt'][y]\n",
    "    zb=constraint_data2['zb'][y]\n",
    "    long=float(\"{0:.2f}\".format(constraint_data2['long_matched'][y]))\n",
    "    lat=float(\"{0:.2f}\".format(constraint_data2['lat_matched'][y]))    \n",
    "    \n",
    "    #Convert datasets into arrays \n",
    "    coords_da1={'longitude': longitude_coords, 'depth':thickness_1} #Create EC array\n",
    "    ec_da = xr.DataArray(EC, dims=('longitude', 'depth'), coords=coords_da1)\n",
    "    \n",
    "    coords_da2={'longitude': longitude_coords, 'layers': numberoflocations}#Create thickness array\n",
    "    thickness_da = xr.DataArray(thicknessoflayers, dims=('longitude', 'layers'), coords=coords_da2)\n",
    "    \n",
    "    coords_da3={'longitude': longitude_coords, 'depth': thickness_1}\n",
    "    latitude_da = xr.DataArray(latitude_array, dims=('longitude','depth'), coords=coords_da3)\n",
    "    \n",
    "    coords_da4={'longitude': longitude_coords, 'depth': thickness_1}\n",
    "    weighting_da = xr.DataArray(constraint_weighting, dims=('longitude','depth'), coords=coords_da4)\n",
    "    \n",
    "    dataset = xr.Dataset({'ec_dataset': ec_da, 'thickness_dataset': thickness_da, 'northing_dataset':latitude_da, 'weighting_dataset':weighting_da}) #Create combined EC and thickness array\n",
    "    \n",
    "    #Change EC and weighting values at certain depths\n",
    "    array2=dataset.assign(ec=EC_value1).where((thickness_da>zt)&(thickness_da<zb)).sel(longitude=long)\n",
    "    array2['ec']=array2.ec.fillna(reference_EC1)\n",
    "    array3=np.array(array2['ec'])\n",
    "    array4=dataset.assign(weighting=hard_constraint).where((thickness_da>zt)&(thickness_da<zb)).sel(longitude=long)   \n",
    "    array4['weighting']=array4.weighting.fillna(no_constraint)\n",
    "    array5=np.array(array4['weighting'])\n",
    "    #Create array with updated EC and weighting values at certain depths\n",
    "    ec_da.loc[long]=array3\n",
    "    weighting_da.loc[long]=array5\n",
    "    dataset = xr.Dataset({'ec_dataset': ec_da, 'thickness_dataset': thickness_da, 'latitude_dataset':latitude_da, 'weighting_dataset':weighting_da}) #Create combined EC and thickness array\n",
    "\n",
    "dataset.sel(longitude=220955.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create and save a .csv that contains depth of investigation data for input into Discover PA software\n",
    "dat_test.to_csv(output_folder+'/depth_data.csv') #save doi data as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #create subset of three dataframes and reformat\n",
    "\n",
    "# #thickness\n",
    "# thickness_1=thicknessoflayers\n",
    "# thickness_2= thicknessoflayers[0:1]\n",
    "# thickness_2=np.array(thickness_2).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create data array of reference constraint values\n",
    "# constraint_weighting=np.full((datlen,layers),no_constraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dat_test=dat.set_index([2,3], drop=False)\n",
    "# dat_test['all_sumcoord']=dat_test[2]+dat_test[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numberoflocations=list(range(layers))\n",
    "# numberoflocations=np.asarray(numberoflocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EC_value1=constraint_data2['v'][0]\n",
    "# zt=constraint_data2['zt'][0]\n",
    "# zb=constraint_data2['zb'][0]\n",
    "# easting=float(\"{0:.2f}\".format(constraint_data2['E_matched'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coords_5=pd.np.tile(coords_4, ((len(coords_4), 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Convert datasets into arrays\n",
    "\n",
    "# #Create EC array\n",
    "# coords1={'easting':coords_2, 'depth':thickness_2}\n",
    "# ec_da = xr.DataArray(EC, dims=('easting', 'depth'), coords=coords1,)\n",
    "\n",
    "# #Create thickness array\n",
    "# coords2={'easting': coords_2, 'layer': [1, 2, 3, 4, 5]}\n",
    "# thickness_da = xr.DataArray(thicknessoflayers, dims=('easting', 'layer'), coords=coords2)\n",
    "\n",
    "# #Create thickness array\n",
    "# coords3={'northing': coords_4, 'depth': thickness_2}\n",
    "# coords_da = xr.DataArray(coords_5, dims=('northing','depth'), coords=coords3)\n",
    "# # coords3={'northing': coords_4, 'depth': thickness_2}\n",
    "# # coords_da = xr.DataArray(thickness_1, dims=('northing','depth'), coords=coords3)\n",
    "\n",
    "\n",
    "# #Create combined EC and thickness array\n",
    "# dataset = xr.Dataset({'ec_dataset': ec_da, 'thickness_dataset': thickness_da, 'northing_dataset':coords_da})\n",
    "# # dataset = xr.Dataset({'ec': ec_da, 'thickness_name': thickness_da, 'thickness_name2':coords_da})\n",
    "\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Change EC values at certain depths\n",
    "# array2=dataset.assign(ec=EC_value1).where((thickness_da>zt)&(thickness_da<zb)).sel(northing=northing).sel(easting=easting)\n",
    "# array2['ec']=array2.ec.fillna(reference_EC1)\n",
    "# array3=np.array(array2['ec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Create array with updated EC values at certain depths\n",
    "# ec_da.loc[easting]=array3\n",
    "# dataset = xr.Dataset({'ec_dataset': ec_da, 'thickness_dataset': thickness_da, 'northing_dataset':coords_da}) #Create combined EC and thickness array\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# alldata=alldata.set_index([2,3], drop=False)\n",
    "# alldata['all_sumcoord']=alldata[2]+alldata[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #join datafile and layer thickness dataframes\n",
    "# alldata=pd.concat([dat,thicknessoflayers,EC], axis=1, join='outer',ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coords_m=dat.iloc[:,1:3]\n",
    "# EC_1= xr.Dataset(EC.iloc[0:9,0:9])\n",
    "# thickness_m=xr.Dataset(thicknessoflayers.iloc[0:9,0:9])\n",
    "# dat_m=xr.Dataset(dat.iloc[0:9,0:9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset = xarray.Dataset({'ec': ec_da, 'thickness_name': thickness_da})\n",
    "# dataset.ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coords={'location': coords_test, 'layer': [1, 2, 3, 4, 5]}\n",
    "# thickness_da = xarray.DataArray(thickness_m, dims=('location', 'layer'), coords=coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# thickness_test= thickness_m[0:1]\n",
    "# thickness_test=np.array(thickness_test).squeeze()\n",
    "# thickness_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coords_test= coords_m.transpose()\n",
    "# coords_test=np.array(coords_test).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset.sel(location=220926.1).update({dataset.ec:('ec2',[0.05,  0.05,  0.01,  0.01,  0.01])}, inplace=True)\n",
    "# dataset.sel(location=220926.1).update({'ec':('ec2',[0.05,  0.05,  0.01,  0.01,  0.01])}, inplace=True)\n",
    "#dataset.location[0]#=dataset.sel(location=220926.1)#.update({dataset.ec:('ec2',[0.05,  0.05,  0.01,  0.01,  0.01])}, inplace=True)\n",
    "# dataset.loc['location',['220926.1']]\n",
    "\n",
    "#dataset.update({'ec':('ec',[1,2,3])})\n",
    "#dataset.update({(dataset.sel(location=220926.1).ec):(1)})\n",
    "# dataset.ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test2= dataset.sel(location=220926.1)\n",
    "# test2=test2.thickness_name<10#.ec\n",
    "# test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset.ec.isel_points(location=(dataset.thickness_name < 10).location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset.where(dataset.sel(location=51).thickness_name < 10).fillna(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset.location[:] = [5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#coords_m=pd.DataFrame(dat.iloc[0:5,2:4])\n",
    "\n",
    "# test=pd.concat([coords_m,thickness_m,EC_m],axis=1,ignore_index=True).set_index([0,1])\n",
    "# test=pd.concat([test,constraint_data2],axis=1)\n",
    "# test \n",
    "\n",
    "\n",
    "# thickness_m=pd.concat([coords_m,thickness_m],axis=1,ignore_index=True)#.set_index([0,1])\n",
    "# array=np.array([[coords_m],[thickness_m],[EC_m]])\n",
    "# array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for row in constraint_data:\n",
    "#     print(row)\n",
    "    #sample[str(row)]= 0\n",
    "\n",
    "#print(sample)\n",
    "    \n",
    "    \n",
    "#     print('loop1_start')\n",
    "#     print(row)\n",
    "#     print('loop1_ends')\n",
    "#     for blah in constraint_data['const_sumcoord']:\n",
    "#         print('loop2_starts')\n",
    "#         print (blah)\n",
    "#         print('loop2_ends')\n",
    "# #         while x>0:\n",
    "#         for rah in sample:\n",
    "#             sample[str(row)]=np.abs(test.all_sumcoord-blah)\n",
    "# #                 x=x-1\n",
    "#             print(sample)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## find coordinate\n",
    "## enter layer thickness (e.g 3)\n",
    "## extrapolate between the points to maximum layers (e.g. 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# array=np.array([[dat],[thicknessoflayers],[EC]],ndmin=3)\n",
    "# array.shape\n",
    "# #test=np.ndarray(array)\n",
    "# array[[0,0,0],[[0,0,0],[0,0,0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #create a dataframe that contains layer thickness saved in confile (.con)\n",
    "# tlayers=con.loc[con[0].str.contains('Thickness      = ')]\n",
    "# thicknessoflayers= tlayers.replace(regex=True,to_replace=r'Thickness      = ', value=r'')\n",
    "# thicknessoflayers=thicknessoflayers[0].str.split((' '),expand=True).replace('',np.nan)\n",
    "# thicknessoflayers.dropna(axis=1,inplace=True)\n",
    "# thicknessoflayers=(pd.np.tile(thicknessoflayers, (datlen, 1)))\n",
    "# #thicknessoflayers=pd.DataFrame(thicknessoflayers)\n",
    "# thicknessoflayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# thicknessoflayers=(pd.np.tile(thicknessoflayers, (datlen, 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def coord(easting,northing):\n",
    "#     easting=int(easting)\n",
    "#     northing=int(northing)\n",
    "#     print(type(easting))\n",
    "\n",
    "# coord('1','2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def print_text():\n",
    "#     print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = [[1, 2, 3, 4], [5, 6], [7, 8, 9]]\n",
    "# for i in range(len(a)):\n",
    "#     for j in range(len(a[i])):\n",
    "#         print(a[i][j], end=' ')\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EC_x=xr.Dataset([EC_m],[thickness_m])\n",
    "# EC_x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
