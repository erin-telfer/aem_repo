{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference model constraint"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Test- how to create reference model for each sounding\n",
    "\n",
    "Requirements:\n",
    "1) .dat\n",
    "2) .con file\n",
    "3) constraint file as csv.\n",
    "\n",
    "To do:\n",
    "\n",
    "Erin Telfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### check constraint value s in sbs code. \n",
    "\n",
    "#### how to spread values out laterally\n",
    "    #### how to relax constraints laterally\n",
    "    \n",
    "    \n",
    "#### how to change the number of layers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User requirement: enter file input information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## User requirement: enter the folder location where the .dat file is saved\n",
    "folder= str(r\"C:/Users/u67397/AnacondaProjects/aem/input_data/\")\n",
    "\n",
    "## User requirement: enter the name of the second .dat file (without the \".dat\" text)\n",
    "dat_name= str(\"AA140005_Line1080\")\n",
    "\n",
    "## User requirement: enter the name of the second .dat file (without the \".dat\" text)\n",
    "con_name= str(\"galeisbstdem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User requirement: enter reference model data variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## User requirement: enter the reference model conductivity specified for result file\n",
    "reference_EC1= 0.001\n",
    "\n",
    "## User requirement: enter the number of layers result file\n",
    "layers= 30\n",
    "\n",
    "##User requirement: set constraint weighting. 1% =no constraint (e.g. reference model) and 100%= hard constraint at location\n",
    "no_constraint=1\n",
    "hard_constraint=10\n",
    "lateral_constraint=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Enter column number within .dat file that contains the longitude and latitude. For example, the third column should be \"3\",\n",
    "#please do not use programmer counting.\n",
    "#coord- easting/northing\n",
    "lat_column= 3\n",
    "long_column= 4\n",
    "\n",
    "##Other variables here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User requirement: enter file output information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## User requirement: enter the folder location where to save files into\n",
    "\n",
    "output_folder= str(r\"C:/Users/u67397/AnacondaProjects/aem/output_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create array that contains contrained data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load .dat file and then process\n",
    "dat= pd.read_fwf(folder+dat_name+\".dat\", header=None) #load dat file\n",
    "dat=dat.set_index([2,3], drop=False) #set coordinates as index\n",
    "dat['all_sumcoord']=dat[2]+dat[3] #create a new column that combines coordinates\n",
    "datlen= len(dat.index) #create variable of dat length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extract thickness data from .dat file and then process\n",
    "con = pd.read_csv (folder+con_name+\".con\", header=None) #import control file\n",
    "con.replace(regex=True,to_replace=r'\\t', value=r'', inplace=True)#remove tabs\n",
    "tlayers=con.loc[con[0].str.contains('Thickness      = ')] #create a dataframe that contains layer thickness saved in confile (.con)\n",
    "thicknessoflayers= tlayers.replace(regex=True,to_replace=r'Thickness      = ', value=r'')\n",
    "thicknessoflayers=thicknessoflayers[0].str.split((' '),expand=True).replace('',np.nan)\n",
    "thicknessoflayers.dropna(axis=1,inplace=True)\n",
    "thicknessoflayers=thicknessoflayers.reset_index(drop=True).T.reset_index(drop=True).T\n",
    "thicknessoflayers=thicknessoflayers.transpose().astype(float)\n",
    "bottomlayer=thicknessoflayers[0].iloc[-1]\n",
    "bottomlayer=np.array([bottomlayer])\n",
    "bottomlayer=pd.DataFrame(bottomlayer)\n",
    "thicknessoflayers=thicknessoflayers.append(bottomlayer).reset_index(drop=True)\n",
    "thicknessoflayers=np.cumsum(thicknessoflayers)\n",
    "thicknessoflayers=thicknessoflayers.transpose()\n",
    "thicknessoflayers=(pd.np.tile(thicknessoflayers, (datlen, 1)))\n",
    "thicknessoflayers=pd.DataFrame(thicknessoflayers)\n",
    "\n",
    "#thickness\n",
    "thickness_1= thicknessoflayers[0:1]\n",
    "thickness_1=np.array(thickness_1).squeeze()\n",
    "\n",
    "numberoflocations=list(range(layers))\n",
    "numberoflocations=np.asarray(numberoflocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create dataframe of reference EC values\n",
    "EC=pd.Series(reference_EC1)\n",
    "EC=(pd.np.tile(EC, (datlen, layers)))\n",
    "EC=pd.DataFrame(EC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>layer_top</th>\n",
       "      <th>layer_bottom</th>\n",
       "      <th>layer_EC</th>\n",
       "      <th>const_sumcoord</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>erin</th>\n",
       "      <td>erin</td>\n",
       "      <td>220955.6</td>\n",
       "      <td>6867443.6</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>7088399.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rules</th>\n",
       "      <td>rules</td>\n",
       "      <td>220956.6</td>\n",
       "      <td>6867444.6</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>7088401.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  longitude   latitude  layer_top  layer_bottom  layer_EC  \\\n",
       "ID                                                                      \n",
       "erin    erin   220955.6  6867443.6          0            10      0.05   \n",
       "rules  rules   220956.6  6867444.6         10            20      0.10   \n",
       "\n",
       "       const_sumcoord  \n",
       "ID                     \n",
       "erin        7088399.2  \n",
       "rules       7088401.2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load constraint file and process \n",
    "constraint_name=str(\"input_test\") \n",
    "constraint_data= pd.read_csv(folder+constraint_name+\".csv\") #import control file\n",
    "constraint_data['const_sumcoord']=constraint_data['longitude']+constraint_data['latitude']\n",
    "constraint_data=constraint_data.set_index(['ID'], drop=False)\n",
    "\n",
    "#create data array of reference constraint values\n",
    "constraint_weighting=np.full((datlen,layers),no_constraint)\n",
    "constraint_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#coordinates\n",
    "longitude_coords=pd.DataFrame(dat.iloc[:,(long_column -2):(long_column -1)])\n",
    "longitude_coords=longitude_coords.transpose()\n",
    "longitude_coords=np.array(longitude_coords).squeeze()\n",
    "\n",
    "latitude_coords=pd.DataFrame(dat.iloc[:,(lat_column -2):(lat_column -1)])\n",
    "latitude_coords=latitude_coords.transpose()\n",
    "latitude_coords=np.array(latitude_coords).squeeze()\n",
    "latitude_array=pd.np.tile(latitude_coords, ((len(numberoflocations), 1))).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matched=pd.DataFrame([0,0,0]).transpose().set_index([0])\n",
    "\n",
    "for x in range(len(constraint_data)):\n",
    "    g=(constraint_data.ID[x])\n",
    "    h=(constraint_data.const_sumcoord[x])\n",
    "    dat[str(g)]= np.abs(dat.all_sumcoord-h) \n",
    "    matched1=np.argmin(dat[str(g)])\n",
    "    e_matched=matched1[0]\n",
    "    n_matched=matched1[1]\n",
    "    matched2=pd.DataFrame([g,float(e_matched),float(n_matched)]).transpose().set_index([0]).astype(float)\n",
    "    matched=pd.concat([matched,matched2], axis=0)\n",
    "\n",
    "matched=matched.rename(columns={1:'long_matched',2:'lat_matched'}).drop([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>layer_top</th>\n",
       "      <th>layer_bottom</th>\n",
       "      <th>layer_EC</th>\n",
       "      <th>const_sumcoord</th>\n",
       "      <th>long_matched</th>\n",
       "      <th>lat_matched</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long_matched</th>\n",
       "      <th>lat_matched</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220955.6</th>\n",
       "      <th>6867443.6</th>\n",
       "      <td>erin</td>\n",
       "      <td>220955.6</td>\n",
       "      <td>6867443.6</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>7088399.2</td>\n",
       "      <td>220955.6</td>\n",
       "      <td>6867443.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220958.6</th>\n",
       "      <th>6867443.7</th>\n",
       "      <td>rules</td>\n",
       "      <td>220956.6</td>\n",
       "      <td>6867444.6</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>7088401.2</td>\n",
       "      <td>220958.6</td>\n",
       "      <td>6867443.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             ID  longitude   latitude  layer_top  \\\n",
       "long_matched lat_matched                                           \n",
       "220955.6     6867443.6     erin   220955.6  6867443.6          0   \n",
       "220958.6     6867443.7    rules   220956.6  6867444.6         10   \n",
       "\n",
       "                          layer_bottom  layer_EC  const_sumcoord  \\\n",
       "long_matched lat_matched                                           \n",
       "220955.6     6867443.6              10      0.05       7088399.2   \n",
       "220958.6     6867443.7              20      0.10       7088401.2   \n",
       "\n",
       "                          long_matched  lat_matched  \n",
       "long_matched lat_matched                             \n",
       "220955.6     6867443.6        220955.6    6867443.6  \n",
       "220958.6     6867443.7        220958.6    6867443.7  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraint_data2= pd.concat([constraint_data, matched],axis=1)\n",
    "constraint_data2=constraint_data2.set_index(['long_matched','lat_matched'],drop=False)\n",
    "constraint_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for y in range(len(constraint_data2)):\n",
    "    EC_value1=constraint_data2['layer_EC'][y]\n",
    "    zt=constraint_data2['layer_top'][y]\n",
    "    zb=constraint_data2['layer_bottom'][y]\n",
    "    long=float(\"{0:.2f}\".format(constraint_data2['long_matched'][y]))\n",
    "    lat=float(\"{0:.2f}\".format(constraint_data2['lat_matched'][y]))    \n",
    "    \n",
    "    #Convert datasets into arrays \n",
    "    coords_da1={'longitude': longitude_coords, 'depth':thickness_1} #Create EC array\n",
    "    ec_da = xr.DataArray(EC, dims=('longitude', 'depth'), coords=coords_da1)\n",
    "    \n",
    "    coords_da2={'longitude': longitude_coords, 'layers': numberoflocations}#Create thickness array\n",
    "    thickness_da = xr.DataArray(thicknessoflayers, dims=('longitude', 'layers'), coords=coords_da2)\n",
    "    \n",
    "    coords_da3={'longitude': longitude_coords, 'depth': thickness_1}\n",
    "    latitude_da = xr.DataArray(latitude_array, dims=('longitude','depth'), coords=coords_da3)\n",
    "    \n",
    "    coords_da4={'longitude': longitude_coords, 'depth': thickness_1}\n",
    "    weighting_da = xr.DataArray(constraint_weighting, dims=('longitude','depth'), coords=coords_da4)\n",
    "    \n",
    "    dataset = xr.Dataset({'ec_dataset': ec_da, 'thickness_dataset': thickness_da, 'northing_dataset':latitude_da, 'weighting_dataset':weighting_da}) #Create combined EC and thickness array\n",
    "    \n",
    "    #Change EC and weighting values at certain depths\n",
    "    array2=dataset.assign(ec=EC_value1).where((thickness_da>zt)&(thickness_da<zb)).sel(longitude=long)\n",
    "    array2['ec']=array2.ec.fillna(reference_EC1)\n",
    "    array3=np.array(array2['ec'])\n",
    "    array4=dataset.assign(weighting=hard_constraint).where((thickness_da>zt)&(thickness_da<zb)).sel(longitude=long) \n",
    "    print()\n",
    "    array4['weighting']=array4.weighting.fillna(no_constraint)\n",
    "    array5=np.array(array4['weighting'])\n",
    "    #Create array with updated EC and weighting values at certain depths\n",
    "    ec_da.loc[long]=array3\n",
    "    weighting_da.loc[long]=array5\n",
    "    dataset = xr.Dataset({'ec_dataset': ec_da, 'thickness_dataset': thickness_da, 'latitude_dataset':latitude_da, 'weighting_dataset':weighting_da}) #Create combined EC and thickness array\n",
    "\n",
    "#dataset.sel(longitude=220958.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighting_da.T\n",
    "# # weighting_da.depth#.sel(longitude=220958.6)\n",
    "# # weighting_da.sel(longitude=220958.6)\n",
    "# weighting_da#.sel(longitude=220958.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weighting_da= weighting_da.data[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test=np.zeros((3,5))\n",
    "# test[:2,2]=1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test=np.zeros((4,5))\n",
    "# test[:2,2]=2 \n",
    "\n",
    "# ix_x,ix_y = np.where(test>0) \n",
    "# print(ixx)\n",
    "# print(ixy)\n",
    "# test[ix,0:2] = 1\n",
    "# test[ix,3:4] = 1\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([5], dtype=int64),)\n",
      "(array([5], dtype=int64),)\n",
      "[[  1.    2.8   4.6   6.4   8.2  10.    1.    1.    1.    1.    1. ]\n",
      " [  1.    2.8   4.6   6.4   8.2  10.    1.    1.    1.    1.    1. ]\n",
      " [  1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1. ]\n",
      " [  1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1. ]\n",
      " [  1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1. ]\n",
      " [  1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1. ]\n",
      " [  1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1. ]\n",
      " [  1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1. ]\n",
      " [  1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1. ]\n",
      " [  1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1. ]\n",
      " [  1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1. ]\n",
      " [  1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1. ]\n",
      " [  1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1. ]\n",
      " [  1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1. ]\n",
      " [  1.    1.    1.    1.    1.    1.    1.    1.    1.    1.    1. ]]\n"
     ]
    }
   ],
   "source": [
    "test=np.zeros((15,11))\n",
    "test[:2,5]=hard_constraint \n",
    "# test[:2,10]=hard_constraint \n",
    "\n",
    "blah_max=np.linspace(hard_constraint,no_constraint,num=lateral_constraint,endpoint=False)\n",
    "blah_min=np.linspace(no_constraint,hard_constraint,num=lateral_constraint,endpoint=False)\n",
    "\n",
    "for row in test:\n",
    "    if row.any()>0:\n",
    "        ixx = np.where(row==hard_constraint) \n",
    "        print(ixx)\n",
    "        ixx= int(ixx[0])\n",
    "        \n",
    "        ixx_min_down=ixx-lateral_constraint\n",
    "        ixx_min_up=ixx\n",
    "            \n",
    "        ixx_max_down=ixx\n",
    "        ixx_max_up=ixx+lateral_constraint\n",
    "        row[ixx_min_down:ixx_min_up]=blah_min\n",
    "#         row[ixx_max_down:ixx_max_up]=blah_max\n",
    "\n",
    "\n",
    "np.place(test,test==0,1)\n",
    "\n",
    "print (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#         for value in np.nditer(test,op_flags=['readwrite']):\n",
    "#             if  value > 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-88-addedfe3037b>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-88-addedfe3037b>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    def test()\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "##create function that \n",
    "def test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (longitude: 20845, depth: 30)>\n",
       "array([[1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       ..., \n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1]])\n",
       "Coordinates:\n",
       "  * longitude  (longitude) float64 2.209e+05 2.209e+05 2.209e+05 2.209e+05 ...\n",
       "  * depth      (depth) float64 3.0 6.3 9.92 13.92 18.32 23.16 28.48 34.32 ..."
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighting_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-f9ef205a8f26>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-16-f9ef205a8f26>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    stop here\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "stop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:            (depth: 30, layers: 30, longitude: 20845)\n",
       "Coordinates:\n",
       "  * longitude          (longitude) float64 2.209e+05 2.209e+05 2.209e+05 ...\n",
       "  * depth              (depth) float64 3.0 6.3 9.92 13.92 18.32 23.16 28.48 ...\n",
       "  * layers             (layers) int32 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ...\n",
       "Data variables:\n",
       "    ec_dataset         (longitude, depth) float64 0.001 0.001 0.001 0.001 ...\n",
       "    thickness_dataset  (longitude, layers) float64 3.0 6.3 9.92 13.92 18.32 ...\n",
       "    latitude_dataset   (longitude, depth) int64 45492 45492 45492 45492 ...\n",
       "    weighting_dataset  (longitude, depth) int32 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dat_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-64ff05853af1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#create and save a .csv that contains depth of investigation data for input into Discover PA software\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdat_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/depth_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#save doi data as csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dat_test' is not defined"
     ]
    }
   ],
   "source": [
    "#create and save a .csv that contains depth of investigation data for input into Discover PA software\n",
    "dat_test.to_csv(output_folder+'/depth_data.csv') #save doi data as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #create subset of three dataframes and reformat\n",
    "\n",
    "# #thickness\n",
    "# thickness_1=thicknessoflayers\n",
    "# thickness_2= thicknessoflayers[0:1]\n",
    "# thickness_2=np.array(thickness_2).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #create data array of reference constraint values\n",
    "# constraint_weighting=np.full((datlen,layers),no_constraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dat_test=dat.set_index([2,3], drop=False)\n",
    "# dat_test['all_sumcoord']=dat_test[2]+dat_test[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# numberoflocations=list(range(layers))\n",
    "# numberoflocations=np.asarray(numberoflocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EC_value1=constraint_data2['v'][0]\n",
    "# zt=constraint_data2['zt'][0]\n",
    "# zb=constraint_data2['zb'][0]\n",
    "# easting=float(\"{0:.2f}\".format(constraint_data2['E_matched'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coords_5=pd.np.tile(coords_4, ((len(coords_4), 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Convert datasets into arrays\n",
    "\n",
    "# #Create EC array\n",
    "# coords1={'easting':coords_2, 'depth':thickness_2}\n",
    "# ec_da = xr.DataArray(EC, dims=('easting', 'depth'), coords=coords1,)\n",
    "\n",
    "# #Create thickness array\n",
    "# coords2={'easting': coords_2, 'layer': [1, 2, 3, 4, 5]}\n",
    "# thickness_da = xr.DataArray(thicknessoflayers, dims=('easting', 'layer'), coords=coords2)\n",
    "\n",
    "# #Create thickness array\n",
    "# coords3={'northing': coords_4, 'depth': thickness_2}\n",
    "# coords_da = xr.DataArray(coords_5, dims=('northing','depth'), coords=coords3)\n",
    "# # coords3={'northing': coords_4, 'depth': thickness_2}\n",
    "# # coords_da = xr.DataArray(thickness_1, dims=('northing','depth'), coords=coords3)\n",
    "\n",
    "\n",
    "# #Create combined EC and thickness array\n",
    "# dataset = xr.Dataset({'ec_dataset': ec_da, 'thickness_dataset': thickness_da, 'northing_dataset':coords_da})\n",
    "# # dataset = xr.Dataset({'ec': ec_da, 'thickness_name': thickness_da, 'thickness_name2':coords_da})\n",
    "\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Change EC values at certain depths\n",
    "# array2=dataset.assign(ec=EC_value1).where((thickness_da>zt)&(thickness_da<zb)).sel(northing=northing).sel(easting=easting)\n",
    "# array2['ec']=array2.ec.fillna(reference_EC1)\n",
    "# array3=np.array(array2['ec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Create array with updated EC values at certain depths\n",
    "# ec_da.loc[easting]=array3\n",
    "# dataset = xr.Dataset({'ec_dataset': ec_da, 'thickness_dataset': thickness_da, 'northing_dataset':coords_da}) #Create combined EC and thickness array\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# alldata=alldata.set_index([2,3], drop=False)\n",
    "# alldata['all_sumcoord']=alldata[2]+alldata[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #join datafile and layer thickness dataframes\n",
    "# alldata=pd.concat([dat,thicknessoflayers,EC], axis=1, join='outer',ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coords_m=dat.iloc[:,1:3]\n",
    "# EC_1= xr.Dataset(EC.iloc[0:9,0:9])\n",
    "# thickness_m=xr.Dataset(thicknessoflayers.iloc[0:9,0:9])\n",
    "# dat_m=xr.Dataset(dat.iloc[0:9,0:9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset = xarray.Dataset({'ec': ec_da, 'thickness_name': thickness_da})\n",
    "# dataset.ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coords={'location': coords_test, 'layer': [1, 2, 3, 4, 5]}\n",
    "# thickness_da = xarray.DataArray(thickness_m, dims=('location', 'layer'), coords=coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# thickness_test= thickness_m[0:1]\n",
    "# thickness_test=np.array(thickness_test).squeeze()\n",
    "# thickness_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coords_test= coords_m.transpose()\n",
    "# coords_test=np.array(coords_test).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset.sel(location=220926.1).update({dataset.ec:('ec2',[0.05,  0.05,  0.01,  0.01,  0.01])}, inplace=True)\n",
    "# dataset.sel(location=220926.1).update({'ec':('ec2',[0.05,  0.05,  0.01,  0.01,  0.01])}, inplace=True)\n",
    "#dataset.location[0]#=dataset.sel(location=220926.1)#.update({dataset.ec:('ec2',[0.05,  0.05,  0.01,  0.01,  0.01])}, inplace=True)\n",
    "# dataset.loc['location',['220926.1']]\n",
    "\n",
    "#dataset.update({'ec':('ec',[1,2,3])})\n",
    "#dataset.update({(dataset.sel(location=220926.1).ec):(1)})\n",
    "# dataset.ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test2= dataset.sel(location=220926.1)\n",
    "# test2=test2.thickness_name<10#.ec\n",
    "# test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset.ec.isel_points(location=(dataset.thickness_name < 10).location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset.where(dataset.sel(location=51).thickness_name < 10).fillna(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset.location[:] = [5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#coords_m=pd.DataFrame(dat.iloc[0:5,2:4])\n",
    "\n",
    "# test=pd.concat([coords_m,thickness_m,EC_m],axis=1,ignore_index=True).set_index([0,1])\n",
    "# test=pd.concat([test,constraint_data2],axis=1)\n",
    "# test \n",
    "\n",
    "\n",
    "# thickness_m=pd.concat([coords_m,thickness_m],axis=1,ignore_index=True)#.set_index([0,1])\n",
    "# array=np.array([[coords_m],[thickness_m],[EC_m]])\n",
    "# array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for row in constraint_data:\n",
    "#     print(row)\n",
    "    #sample[str(row)]= 0\n",
    "\n",
    "#print(sample)\n",
    "    \n",
    "    \n",
    "#     print('loop1_start')\n",
    "#     print(row)\n",
    "#     print('loop1_ends')\n",
    "#     for blah in constraint_data['const_sumcoord']:\n",
    "#         print('loop2_starts')\n",
    "#         print (blah)\n",
    "#         print('loop2_ends')\n",
    "# #         while x>0:\n",
    "#         for rah in sample:\n",
    "#             sample[str(row)]=np.abs(test.all_sumcoord-blah)\n",
    "# #                 x=x-1\n",
    "#             print(sample)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## find coordinate\n",
    "## enter layer thickness (e.g 3)\n",
    "## extrapolate between the points to maximum layers (e.g. 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# array=np.array([[dat],[thicknessoflayers],[EC]],ndmin=3)\n",
    "# array.shape\n",
    "# #test=np.ndarray(array)\n",
    "# array[[0,0,0],[[0,0,0],[0,0,0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #create a dataframe that contains layer thickness saved in confile (.con)\n",
    "# tlayers=con.loc[con[0].str.contains('Thickness      = ')]\n",
    "# thicknessoflayers= tlayers.replace(regex=True,to_replace=r'Thickness      = ', value=r'')\n",
    "# thicknessoflayers=thicknessoflayers[0].str.split((' '),expand=True).replace('',np.nan)\n",
    "# thicknessoflayers.dropna(axis=1,inplace=True)\n",
    "# thicknessoflayers=(pd.np.tile(thicknessoflayers, (datlen, 1)))\n",
    "# #thicknessoflayers=pd.DataFrame(thicknessoflayers)\n",
    "# thicknessoflayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# thicknessoflayers=(pd.np.tile(thicknessoflayers, (datlen, 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def coord(easting,northing):\n",
    "#     easting=int(easting)\n",
    "#     northing=int(northing)\n",
    "#     print(type(easting))\n",
    "\n",
    "# coord('1','2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def print_text():\n",
    "#     print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = [[1, 2, 3, 4], [5, 6], [7, 8, 9]]\n",
    "# for i in range(len(a)):\n",
    "#     for j in range(len(a[i])):\n",
    "#         print(a[i][j], end=' ')\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EC_x=xr.Dataset([EC_m],[thickness_m])\n",
    "# EC_x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
